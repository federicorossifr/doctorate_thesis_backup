\chapter{Introduction}
\lettrine{I}{n} the latest years, the use of deep neural networks (DNNs) -- and, in general, Artificial Intelligence (AI) -- as a general tool for signal and data processing has been pervasive, with a strong presence of both in industry and academia. The effort in computing these artificial intelligence algorithms is an open challenge in the field of computing platforms nowadays. In particular, when considering strict requirements, such as lowering the power consumption, maximizing the throughput and minimizing the latency the computational complexity becomes more and more critical.


With real-time scenarios putting growing limits on the use of Deep Neural Networks (DNNs), there is a need to reassess information representation. A difficult path is to use an encoding that enables quick processing and hardware-friendly information representation. Among the suggested alternatives to the IEEE 754 standard for floating point representation of real numbers, the recently developed Posit format has been theoretically demonstrated to be extremely promising in meeting the aforementioned requirements. However, due to the lack of sufficient hardware support for this new type, this evaluation could initially only be carried out via software emulation.

Even without hardware acceleration, we can use the Posit format and the currently available arithmetic logic units in processors to accelerate DNNs by manipulating the low-level bit string representations of posits. In this paper, we introduce new arithmetic features of the Posit number system with an emphasis on the configuration with 0 exponent bits as a first step. We propose a new class of Posit operators dubbed L1 operators, which are fast and approximated versions of current arithmetic operations or functions (for example, hyperbolic tangent (TANH) and extended linear unit (ELU)) that only use integer arithmetic. These operators introduce very interesting properties and results: (i)faster evaluation than the non-approximated counterpart with negligible accuracy degradation; (ii) efficient ALU emulation of a number of Posits operations; (iii) the ability to vectorize operations n Posits using existing ALU vectorized operations; and (iv) the ability to vectorize operations n Posits using existing ALU vectorized operations (such as the Scalable Vector Extension of ARM CPUs or Advanced Vector Extensions on Intel CPUs). Furthermore, we test the proposed activation function on Posit-based DNNs, demonstrating how 16-bit down to 10-bit Posits represent an exact replacement for 32-bit floats. In contrast, 8-bit Posits could be an interesting alternative to 32-bit Floats because their performance is slightly lower. Still, their high speed and low storage properties are very appealing (leading to a lower bandwidth demanding and more cache-friendly code).

On the hardware side, RISC-V recently rose as an open-source alternative CPU architecture \cite{riscvisa,waterman2011risc,asanovic2014instruction}. It quickly became an essential competitor of Intel, AMD and ARM CPUs (both for 32 and 64-bit variants) for being royalty-free. Several companies in the industry have already supported and funded the project. Among them, we can find star companies like Intel, Microsoft and ST Microelectronics \cite{riscvabout}. The most important and crucial feature of RISC-V is its open-source instruction set architecture (ISA).
This means that anyone can modify it by extending the ISA with his very own instructions and functionalities: this feature is fundamental since it allows the design of very low-latency co-processors, functional units and accelerators without the need to consider them as external devices that require memory mapping and interrupt.

We sought to add as minimal instructions as possible to the RISC-V architecture in order to conduct weight compression and decompression using the posit format for storage. It is obvious that, because we did not wish to completely replace floats, having an interchange and compressed format for transporting information is advantageous. Furthermore, we analyze what could happen when we employ this compression in a ``real-time" approach, and not only on the storage. Of course, adding additional instructions impacted the performance, since the effort of full-posit processing cannot stand without full-hardware support for posits (i.e. full PPU with all the arithmetic operations). Therefore, finally, we moved to a full-ppu design that includes all the arithmetic operations alongside the conversion procedures already implemented.

The work is organised as follows: 
\begin{itemize}
    \item Chapter \ref{chap:real_reps}: a review of state-of-the-art real number representations with a novel proposed approaches to encode real numbers. We also present the properties of the different formats highlighting their advantages and disadvantages. 
    \item Chapter \ref{chap:posit_num}: an in-depth review of the posit format, starting from the work of John L. Gustafson with original findings from this work. We highlight all the key points of the formats as well as their arithmetical properties in comparison with the standard real number representations. Furthermore, in Section \ref{sec:posit_ops} we deepen on the arithmetic/logic aspects of binary operations between posits (algebraic sum, multiplication and division), with a particular focus on the implementation of the division operation.
    \item Chapter \ref{chap:posit_library}: details on the implementation of a cross-platform posit library used to offer a high-level application programming interface to operate with posit numbers. We also report the implementation of vectorized kernels for posit encoding and decoding in vector processors like ARM SVE or RISC-V "V".
    \item Chapter \ref{chap:posit_networks}: details on the use of posit numbers inside deep neural networks as well as our implementation of deep neural networks that use posit as computing elements. In particular, we present the integration of the posit library inside the Tensorflow DNN library and related results on DNN tasks.
    \item Chapter \ref{chap:posit_hw}: first hardware implementation of a posit processing unit, in a lightweight fashion, only implementing conversion. We also include the integration inside a RISC-V core with related instruction encodings.
    \item Chapter \ref{chap:posit_fullppu}: in-depth review and implementation of full posit processing units. We also characterise the unit reasoning on the impact of pipeline-ing to the component latency and the benefits of different approaches to operation implementation such as division.
    
\end{itemize}


\section{Acknowledgments}
The work contained in this thesis was carried out with the support and alongside  the following  frameworks:
\begin{itemize}
    \item European Processor Initiative (EPI, SGA1, SGA2): \\\url{https://www.european-processor-initiative.eu}
    \item Towards EXtreme scale Technologies and Accelerators for euROhpc hw/Sw Supercomputing Applications for exascale (TEXTAROSSA): \\\url{https://textarossa.eu}
    \item Pilot using Independent Local \& Open Technologies (EUPILOT, The European PILOT): \\\url{https://eupilot.eu/}
\end{itemize}