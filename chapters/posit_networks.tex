
\chapter{Deep Neural Networks and posits}\label{chap:posit_networks}
\section{Tailoring neural networks and posits together}

\lettrine{W}{hen} considering posit numbers for DNNs, we need to take into account that the highest density of posit numbers is in the range $[-useed,useed]$. This range indeed represents half of the posit projective circle. This can be exploited to design networks that are more proficient when used together with posit numbers. This can be addressed in different ways, as discussed in the next subsections.

\subsection{Activation functions} When choosing activation functions we need to consider the output range of the functions. For example, if we consider the ReLU activation function, it discards all the negative numbers passed as argument flattening them to $0$. Furthermore the sigmoid function, limiting the output in $[0,1]$ discards the precious high-density region $[-1,0]$. Instead, the hyperbolic tangent can fully exploit the region $[-1,1]$. However, being modern deep neural network architectures are very deep (the number of layers is huge), S-shaped functions like hyperbolic tangents suffer from vanishing gradients, thus they are not acceptable in the training process. The ELU function and in general Scaled Extended Linear Units (SELUs, \cite{NIPS2017_6698}), manages to cover a higher range, typically parameterized by two real factors $\alpha$ and $\beta$ : $[-\alpha \cdot \beta,+\infty]$.

\subsection{Distribution of values} When stacking layers in a deep model we need to care about the right-skewing of value distributions during forwarding passes. Adding a batch normalization layer \cite{Ioffe:2015:BNA:3045118.3045167} after some convolution and activation steps can manage to re-scale the values by subtracting the batch mean and dividing it by its standard deviation. This will result in a value distribution with a null mean value and unitary standard deviation, thus fitting the needs already explained. 

\subsection{Loss strategies} If we want to perform low-precision inference without losing too much accuracy (e.g. switching to posit$\left <8,0\right>$ for inference) we may need to take into account the dynamic range of such types (e.g. posit$\left <8,0\right>$ has a range $[-64,64]$). This means that, during the training, we must penalise high network weights. This can be addressed by using different types of regularization. In \cite{kukaka2017regularization}  are shown recent trends in regularization for neural networks. For example a weight decay approach (see \cite{plaut1986experiments}) with a decay rate of $\lambda$ adds the following L2 regularization term to the loss: \[R(w) = \lambda \cdot \frac{1}{2} \cdot \left | w \right |^2_2 . \] This has been proven to reduce overfit and training error in \cite{Krizhevsky:2017:ICD:3098997.3065386}. In general, avoiding overfitting can help in maintaining low weight values. Therefore, the use of other layers designed to help with a generalization like a dropout layer \cite{JMLR:v15:srivastava14a} can be useful as well.


\subsection{Data pre-processing} When considering low precision inference, we also need to take into account the encoding of data fed to the neural network. For example, if we take an RGB dataset, we will find each pixel encoded in each channel as an integer in $[0,255]$. If we feed this type of data to a posit$\left <8,0\right>$ network, it will result in values above $64$ being clipped down to the maximum value. Moreover, we are not exploiting the negative axis. To address this problem, we may apply a re-scaling of the encoding before even training the network. Simply re-scaling the image in $[-1,1]$ is not always a good solution, since it may result in an unacceptable loss of information. Another important point in the posit circle is the $useed = \pm 2^{2^{es}}$ point, which is strictly connected to the dynamic range of a posit $\pm useed^{nbits-2}$. For example, re-scaling an image in the range $[-useed,useed]$ of posit$\left <8,0\right>$ (thus having the pixel encoded in $[-2,2]$), has been proven to be an effective encoding during both training (with higher precision types) and inference phase (with low precision types). The formula to re-scale the value $p$ of each pixel is therefore: \[ n(p) = 2\cdot useed \cdot \frac{p}{255} - useed. \]

\section{Results on deep neural networks}

In this section we present several results on the use of Posits in deep neural networks; indeed, to prove the capability of posit to be able to \textit{replace} IEEE binary32 numbers in such applications, we integrated the cppPosit library within two neural network libraries:
\begin{itemize}
    \item tinyDNN\footnote{\url{https://github.com/tiny-dnn/tiny-dnn}}: a minimal C++ header-only library, very useful for debugging the posit format.
    \item Tensorflow\footnote{\url{https://www.tensorflow.org}}: one of the most common neural network libraries with a wide proposal of pre-trained models and applications.
\end{itemize}

We used the following network models:
\begin{itemize}
    \item LeNet-5 like convolutional neural network \cite{lecunlenet}, \
    \item EfficientNet deep convolutional neural network \cite{tan2020efficientdet}
    \item Single Shot Detector (with $300\times 300$ input images) \cite{Liu_2016}
\end{itemize}

We used the following evaluation datasets:
\begin{itemize}
    \item MNIST: hand-written digits recognition benchmark \cite{lecun-mnisthandwrittendigit-2010}, $32\times32$ grey scale images
    \item GTSRB: German Traffic Sign Recognition benchmark \cite{stallkamp2011gtrsb}, $32\times32$ RGB images
    \item CIFAR10: general purpose image recognition benchmark \cite{Krizhevsky09learningmultiple}, $32\times32$ RGB images
    \item ImagenetV2: additional test-set that uses the same Imagenet classes but with new images \cite{recht2019imagenet}, $224\times224$ RGB images
    \item Pascal VOC 2007: object detection dataset \cite{pascal-voc-2007}, $300\times300$ RGB images
\end{itemize}


\begin{table}[H]
\centering
\caption{Inference (test-set) accuracy on small, edge convolutional neural network trained with binary32, on different small datasets.}
\vspace{1em}
\begin{tabular}{lccc}
\hline
\multicolumn{1}{l}{} & \multicolumn{3}{c}{LeNet-5 like CNN} \\ 
\multicolumn{1}{l}{} & MNIST      & GTRSB      & CIFAR10    \\ \hline
binary32              & 98.86\%    & 91.9\%     & 83.5\%     \\
\posit{16}{1}            & 98.83\%    & 91.8\%     & 83.5\%     \\
p\posit{16}{0}            & 98.50\%    & 90.5\%     & 83\%       \\
bfloat16             & 98.86\%    & 91.9\%     & 82\%       \\
\posit{8}{0}             & 98.34\%    & 90.4\%     & 78\%       \\
bfloat8              & 69.57\%    & 80.45\%    & 67.5\%     \\ \hline
\end{tabular}
\label{tab:small}
\end{table}

\begin{table}[H]
\centering
\caption{Inference accuracy test on very deep neural networks with big datasets (again, pre-trained using binary32).}
\vspace{1em}
\label{tab:big_perf}
\begin{tabular}{lcc}
\hline
\multicolumn{1}{c}{} & \begin{tabular}[c]{@{}c@{}}EfficientNetB0 + ImagenetV2\\ (accuracy)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SSD300 + VOC 2007\\ (mean avg. precision)\end{tabular} \\ \hline
binary32              & 81.9\%                                                                           & 80.39\%                                                                           \\
\posit{16}{2}            & 79.7\%                                                                           & 78.49\%                                                                           \\
bfloat16             & 78.9\%                                                                           & 73.29\%                                                                           \\ \hline
\end{tabular}
\end{table}
